{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":8804697,"sourceType":"datasetVersion","datasetId":5295213},{"sourceId":8811741,"sourceType":"datasetVersion","datasetId":5300370}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data & Config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pydicom\nimport numpy as np\nimport cv2\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchmetrics import Accuracy, F1Score, Recall, AUROC\n\nimport albumentations as A\nimport timm\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-28T14:30:29.204468Z","iopub.execute_input":"2024-06-28T14:30:29.204809Z","iopub.status.idle":"2024-06-28T14:30:39.690389Z","shell.execute_reply.started":"2024-06-28T14:30:29.204783Z","shell.execute_reply":"2024-06-28T14:30:39.689422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (512, 512)\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = TRAIN_BATCH_SIZE * 2\nAUG_PROB = 0.75\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLR = 1e-4\nEPOCHS = 20\nLOAD_CHECKPOINT = True","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:43.151590Z","iopub.execute_input":"2024-06-28T14:30:43.151927Z","iopub.status.idle":"2024-06-28T14:30:43.178295Z","shell.execute_reply.started":"2024-06-28T14:30:43.151903Z","shell.execute_reply":"2024-06-28T14:30:43.177229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/rsna-lsdc-data/train.csv\")\n\ndf = df[df[\"series_description\"] == \"Sagittal T1\"]\n\ncols_to_include = [\"study_id\", \"series_id\", \"instance_number\"] + list(filter(lambda x: x.startswith(\"spinal_canal_stenosis\"), df.columns))\ndf = df[cols_to_include]\n\ndf.reset_index(drop=True, inplace=True)\ndf = pd.get_dummies(df, dtype=int)\n\nNUM_CLASSES = df.shape[1] - 3\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:45.330637Z","iopub.execute_input":"2024-06-28T14:30:45.330988Z","iopub.status.idle":"2024-06-28T14:30:46.594669Z","shell.execute_reply.started":"2024-06-28T14:30:45.330962Z","shell.execute_reply":"2024-06-28T14:30:46.593717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(n=5000, ignore_index=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:48.031810Z","iopub.execute_input":"2024-06-28T14:30:48.032540Z","iopub.status.idle":"2024-06-28T14:30:48.040360Z","shell.execute_reply.started":"2024-06-28T14:30:48.032510Z","shell.execute_reply":"2024-06-28T14:30:48.039421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def load_dicom(src_path, resize_shape):\n\n    dicom_data = pydicom.dcmread(src_path).pixel_array\n    resized_image = (dicom_data / np.max(dicom_data) * 255).astype(np.uint8)\n    resized_image = cv2.resize(resized_image, resize_shape)\n    \n    return resized_image","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:49.782759Z","iopub.execute_input":"2024-06-28T14:30:49.783109Z","iopub.status.idle":"2024-06-28T14:30:49.788523Z","shell.execute_reply.started":"2024-06-28T14:30:49.783083Z","shell.execute_reply":"2024-06-28T14:30:49.787601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, valid_df = train_test_split(df, test_size=.2, random_state=42)\ntrain_df.reset_index(drop=True, inplace=True)\nvalid_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:51.534907Z","iopub.execute_input":"2024-06-28T14:30:51.535581Z","iopub.status.idle":"2024-06-28T14:30:51.543793Z","shell.execute_reply.started":"2024-06-28T14:30:51.535547Z","shell.execute_reply":"2024-06-28T14:30:51.542742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_train = A.Compose([\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUG_PROB),\n    A.OneOf([\n        A.MotionBlur(blur_limit=5),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=5),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n    ], p=AUG_PROB),\n\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=1.0),\n        A.GridDistortion(num_steps=5, distort_limit=1.),\n        A.ElasticTransform(alpha=3),\n    ], p=AUG_PROB),\n\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUG_PROB),\n    # A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n    A.CoarseDropout(max_holes=16, max_height=64, max_width=64, min_holes=1, min_height=8, min_width=8, p=AUG_PROB),    \n    A.Normalize(mean=0.5, std=0.5)\n])\n\ntransforms_valid = A.Compose([\n    # A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n    A.Normalize(mean=0.5, std=0.5)\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:53.180516Z","iopub.execute_input":"2024-06-28T14:30:53.181325Z","iopub.status.idle":"2024-06-28T14:30:53.194542Z","shell.execute_reply.started":"2024-06-28T14:30:53.181285Z","shell.execute_reply":"2024-06-28T14:30:53.193630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSDCDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self): return len(self.df)\n    \n    def __getitem__(self, i):\n        \n        row = self.df.loc[i]\n        study_id = row[\"study_id\"]\n        series_id = row[\"series_id\"]\n        instance_number = row[\"instance_number\"]\n        \n        dcm_src = f\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/{study_id}/{series_id}/{instance_number}.dcm\"\n        image = load_dicom(dcm_src, IMG_SIZE)\n        \n        if self.transforms is not None:\n            image = self.transforms(image=image)[\"image\"]\n        image = torch.tensor(np.expand_dims(image, axis=0), dtype=torch.float)\n        \n        columns = list(self.df.columns)[3:]\n        target = torch.tensor(row[columns].values, dtype=torch.float)\n        \n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:55.987158Z","iopub.execute_input":"2024-06-28T14:30:55.987542Z","iopub.status.idle":"2024-06-28T14:30:55.995616Z","shell.execute_reply.started":"2024-06-28T14:30:55.987511Z","shell.execute_reply":"2024-06-28T14:30:55.994673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LSDCDataset(train_df, transforms=transforms_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n\nvalid_dataset = LSDCDataset(valid_df, transforms=transforms_valid)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:30:58.725089Z","iopub.execute_input":"2024-06-28T14:30:58.725449Z","iopub.status.idle":"2024-06-28T14:30:58.730696Z","shell.execute_reply.started":"2024-06-28T14:30:58.725423Z","shell.execute_reply":"2024-06-28T14:30:58.729837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_classes):\n        super(Model, self).__init__()\n        \n        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)\n        self.cnn_model = timm.create_model('efficientnet_b0', num_classes=num_classes, pretrained=True)\n    \n    def forward(self, X):\n        X = self.conv_layer(X)\n        X = self.cnn_model(X)\n        \n        return X","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:31:01.067628Z","iopub.execute_input":"2024-06-28T14:31:01.068565Z","iopub.status.idle":"2024-06-28T14:31:01.076531Z","shell.execute_reply.started":"2024-06-28T14:31:01.068523Z","shell.execute_reply":"2024-06-28T14:31:01.075389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model = Model(NUM_CLASSES)\nmodel.to(DEVICE)\n\noptimizer = optim.Adam(model.parameters(), lr=LR)\nloss_fn = nn.CrossEntropyLoss()\nscaler = GradScaler()\n\naccuracy = Accuracy(task=\"binary\").to(DEVICE)\nf1 = F1Score(task=\"binary\").to(DEVICE)\nrecall = Recall(task=\"binary\").to(DEVICE)\nauroc = AUROC(task=\"binary\").to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:31:04.597664Z","iopub.execute_input":"2024-06-28T14:31:04.598325Z","iopub.status.idle":"2024-06-28T14:31:06.754571Z","shell.execute_reply.started":"2024-06-28T14:31:04.598295Z","shell.execute_reply":"2024-06-28T14:31:06.753570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_CHECKPOINT:\n    checkpoint_path = \"/kaggle/input/rsna-lsdc-models/0.2/10.pth\"\n    checkpoint = torch.load(checkpoint_path)\n\n    model.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:33:15.243054Z","iopub.execute_input":"2024-06-28T14:33:15.243642Z","iopub.status.idle":"2024-06-28T14:33:15.326102Z","shell.execute_reply.started":"2024-06-28T14:33:15.243612Z","shell.execute_reply":"2024-06-28T14:33:15.325158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(epoch, model, dataloader, loss_fn, optimizer, scaler):\n    model.train()\n    running_loss = .0\n    running_accuracy = .0\n    running_f1 = .0\n    running_recall = .0\n    running_auroc = .0\n    \n    print(f\"Epoch [{epoch}/{EPOCHS}]\")\n    \n    progress_bar = tqdm(dataloader, desc=\"Training\", total=len(dataloader), unit=\"batch\")\n    for X, y in progress_bar:\n        X, y = X.to(DEVICE), y.to(DEVICE)\n        \n        with autocast():\n            output = model(X)\n            loss = loss_fn(output, y)\n            \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        accuracy_score = accuracy(output, y).item()\n        f1_score = f1(output, y).item()\n        recall_score = recall(output, y).item()\n        auroc_score = auroc(output, y).item()\n        \n        running_loss += loss.item() * X.size(0)\n        running_accuracy += accuracy_score * X.size(0)\n        running_f1 += f1_score * X.size(0)\n        running_recall += recall_score * X.size(0)\n        running_auroc += auroc_score * X.size(0)\n        \n        metrics_dict = {\n            \"Batch Loss\": loss.item(),\n            \"Accuracy\": accuracy_score,\n            \"F1\": f1_score,\n            \"Recall\": recall_score,\n            \"AUCROC\": auroc_score\n        }\n        progress_bar.set_postfix(metrics_dict)\n        \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n    epoch_f1 = running_f1 / len(dataloader.dataset)\n    epoch_recall = running_recall / len(dataloader.dataset)\n    epoch_auroc = running_auroc / len(dataloader.dataset)\n    \n    return epoch_loss, epoch_accuracy, epoch_f1, epoch_recall, epoch_auroc","metadata":{"execution":{"iopub.status.busy":"2024-06-28T14:33:29.445176Z","iopub.execute_input":"2024-06-28T14:33:29.445544Z","iopub.status.idle":"2024-06-28T14:33:29.457619Z","shell.execute_reply.started":"2024-06-28T14:33:29.445515Z","shell.execute_reply":"2024-06-28T14:33:29.456540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_epoch(epoch, model, dataloader, loss_fn):\n    model.eval()\n    running_loss = .0\n    running_accuracy = .0\n    running_f1 = .0\n    running_recall = .0\n    running_auroc = .0\n    \n    progress_bar = tqdm(dataloader, desc=\"Validation\", total=len(dataloader), unit=\"batch\")\n    with torch.inference_mode():\n        for X, y in progress_bar:\n            X, y = X.to(DEVICE), y.to(DEVICE)\n            \n            with autocast():\n                output = model(X)\n                loss = loss_fn(output, y)\n                \n            accuracy_score = accuracy(output, y).item()\n            f1_score = f1(output, y).item()\n            recall_score = recall(output, y).item()\n            auroc_score = auroc(output, y).item()\n\n            running_loss += loss.item() * X.size(0)\n            running_loss += loss.item() * X.size(0)\n            running_accuracy += accuracy_score * X.size(0)\n            running_f1 += f1_score * X.size(0)\n            running_recall += recall_score * X.size(0)\n            running_auroc += auroc_score * X.size(0)\n\n            metrics_dict = {\n                \"Batch Loss\": loss.item(),\n                \"Accuracy\": accuracy_score,\n                \"F1\": f1_score,\n                \"Recall\": recall_score,\n                \"AUCROC\": auroc_score\n            }\n            progress_bar.set_postfix(metrics_dict)\n    \n    epoch_loss = running_loss / len(dataloader.dataset)\n    epoch_accuracy = running_accuracy / len(dataloader.dataset)\n    epoch_f1 = running_f1 / len(dataloader.dataset)\n    epoch_recall = running_recall / len(dataloader.dataset)\n    epoch_auroc = running_auroc / len(dataloader.dataset)\n    \n    return epoch_loss, epoch_accuracy, epoch_f1, epoch_recall, epoch_auroc","metadata":{"execution":{"iopub.status.busy":"2024-06-28T01:13:09.133973Z","iopub.execute_input":"2024-06-28T01:13:09.134346Z","iopub.status.idle":"2024-06-28T01:13:09.145463Z","shell.execute_reply.started":"2024-06-28T01:13:09.134315Z","shell.execute_reply":"2024-06-28T01:13:09.144547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"saved-models\", exist_ok=True)\n\ndef save_model(model, epoch):\n    PATH = f\"saved-models/{epoch}.pth\"\n    torch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T01:13:10.128612Z","iopub.execute_input":"2024-06-28T01:13:10.129003Z","iopub.status.idle":"2024-06-28T01:13:10.134050Z","shell.execute_reply.started":"2024-06-28T01:13:10.128972Z","shell.execute_reply":"2024-06-28T01:13:10.133081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(11, EPOCHS+11):\n    train_epoch_loss = train_epoch(epoch, model, train_dataloader, loss_fn, optimizer, scaler)\n    valid_epoch_loss = valid_epoch(epoch, model, valid_dataloader, loss_fn)\n    \n    print(f\"Training Loss - {train_epoch_loss}\")\n    print(f\"Validation Loss - {valid_epoch_loss}\")\n    \n    save_model(model, epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}